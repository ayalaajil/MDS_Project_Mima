{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:08:15.489247: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-05 22:08:15.499836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743883695.513566 3994732 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743883695.517916 3994732 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 22:08:15.531384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import ast\n",
    "from extractor import ExtractingPrompt\n",
    "from LLM import LLM\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/laajila/mima_newcode/clean_code/Final_dataset_generated_one_symptom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcae = pd.read_excel('PRO-CTCAE_Questionnaire_Terminology.xls', sheet_name = 'PRO')\n",
    "symptoms_list = ctcae['PRO-CTCAE PT'].unique()[:-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symptom_scores(output_str):\n",
    "    # This pattern matches a key enclosed in single or double quotes followed by a colon and a number (integer or float)\n",
    "    pattern = r'[\"\\']([^\"\\']+)[\"\\']\\s*:\\s*([0-9]*\\.?[0-9]+)'\n",
    "    matches = re.findall(pattern, output_str)\n",
    "    # Convert the extracted values to float and build the dictionary\n",
    "    return {key: float(value) for key, value in matches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df_results):\n",
    "        \"\"\"\n",
    "        Calculate accuracy of symptom extraction, case-insensitive and with proper handling of extracted symptoms.\n",
    "        \n",
    "        Args:\n",
    "            df_results: DataFrame containing columns 'True_Symptom' and 'Extracted_Symptom'\n",
    "            \n",
    "        Returns:\n",
    "            accuracy: Float between 0 and 1 representing match accuracy\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        total = len(df_results)\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0.0  # handle empty dataframe case\n",
    "        \n",
    "        for i in range(total):\n",
    "            true_symptom = str(df_results.iloc[i]['True_Symptom']).strip().lower()\n",
    "            \n",
    "            try:\n",
    "                # Safely handle the extracted symptom (assuming it might be a string representation of a list)\n",
    "                extracted = df_results.iloc[i]['Extracted_Symptom']\n",
    "                \n",
    "                # Handle different possible formats:\n",
    "                if isinstance(extracted, str):\n",
    "\n",
    "                    # Try to evaluate if it's a string representation of a list\n",
    "                    if extracted.startswith('[') and extracted.endswith(']'):\n",
    "                        extracted_list = [s.strip().lower() for s in extracted[1:-1].split(',')]\n",
    "                    else:\n",
    "                        extracted_list = [extracted.strip().lower()]\n",
    "\n",
    "                elif isinstance(extracted, (list, tuple)):\n",
    "                    extracted_list = [str(s).strip().lower() for s in extracted]\n",
    "                    \n",
    "                else:\n",
    "                    extracted_list = [str(extracted).strip().lower()]\n",
    "                    \n",
    "                # Check if true symptom exists in extracted list\n",
    "                if true_symptom in extracted_list:\n",
    "                    score += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        accuracy = score / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractingPrompt:\n",
    "    \"\"\"\n",
    "    A class to generate prompts for extracting structured symptoms from patient dialogues.\n",
    "    \"\"\"\n",
    "    def __init__(self, symptom_list: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the prompt generator with a predefined list of symptoms.\n",
    "        \"\"\"\n",
    "        self.symptom_list = symptom_list\n",
    "\n",
    "    def build_extraction_prompt(self, dialogue: str) -> list[dict[str, str]]:\n",
    "        \n",
    "        symptoms_str = \", \".join(self.symptom_list)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a medical symptom extraction expert. Your task is to analyze patient dialogues \"\n",
    "\n",
    "                    \"You are an AI assistant specialized in extracting medical symptoms. \"\n",
    "                    \"Given a patient dialogue, identify which symptoms from the provided list are mentioned with high precision using these rules:\\n\"\n",
    "                    \" STRICT MATCHING: Only output symptoms that are mentioned in the symptoms list {symptoms_str}.\\n\"\n",
    "                    \" CONTEXT AWARENESS: Consider negation (e.g., 'no fever') and temporal aspects (e.g., 'had headache yesterday')\\n\"\n",
    "                    \" CONFIDENCE SCORING: Assign scores using this scale:\\n\"\n",
    "                    \"   - 0.9-1.0: Explicitly mentioned (e.g., 'I have fever')\\n\"\n",
    "                    \"   - 0.7-0.8: Strongly implied (e.g., 'my head is pounding' â†’ headache)\\n\"\n",
    "                    \"   - 0.5-0.6: Weakly implied (only include if no better matches exist)\\n\"\n",
    "                    \"4. OUTPUT FORMAT: Strict JSON with {symptom: score} pairs, no additional text\\n\\n\"\n",
    "                    \"Example Output:\\n\"\n",
    "                    \"{\\\"fever\\\": 0.95, \\\"headache\\\": 0.8}\\n\\n\"\n",
    "                    \"Available symptoms: \" + symptoms_str\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"DIALOGUE:\\n\\\"\\\"\\\"\\n\" + dialogue.strip() + \"\\n\\\"\\\"\\\"\\n\\n\"\n",
    "                    \"Analyze this dialogue and extract symptoms according to the rules above. \"\n",
    "                    \"Output ONLY valid JSON with no additional commentary.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the LLM model : Biollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9927daff836424598f14b96c16c3693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "model = LLM(model_name=\"iRASC/BioLlama-Ko-8B\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:19,  2.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "1313it [47:38,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "extractor = ExtractingPrompt(symptoms_list)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, phrase in tqdm(enumerate(df['Dialogue_Generated'])):\n",
    "\n",
    "    prompt = extractor.build_extraction_prompt(phrase)\n",
    "\n",
    "    symptoms_extracted_llm = model.generate_text(messages=prompt)\n",
    "\n",
    "    # Extract symptoms with scores > 0.80\n",
    "    symptom_scores = extract_symptom_scores(symptoms_extracted_llm)\n",
    "\n",
    "    symptoms_extracted = [symptom for symptom, score in symptom_scores.items() if score > 0.80]\n",
    "    \n",
    "\n",
    "    # symptoms_extracted = [list(extract_symptom_scores(symptoms_extracted_llm).keys())[0] ] + [el for el in\n",
    "    #                      list(extract_symptom_scores(symptoms_extracted_llm).keys())[1:] if extract_symptom_scores(symptoms_extracted_llm)[el] > 0.80 ]\n",
    "\n",
    "        \n",
    "    # Format the output appropriately\n",
    "    if len(symptoms_extracted) == 0:\n",
    "        formatted_symptoms = None  # or \"\" if you prefer empty string\n",
    "    elif len(symptoms_extracted) == 1:\n",
    "        formatted_symptoms = symptoms_extracted[0]  # single symptom as string\n",
    "    else:\n",
    "        formatted_symptoms = \", \".join(symptoms_extracted)  # multiple as comma-separated string\n",
    "    \n",
    "\n",
    "    true_symptom = df['Symptom'][i]\n",
    "    \n",
    "    results.append({\n",
    "        \"Dialogue\": phrase,\n",
    "        \"True_Symptom\": true_symptom,\n",
    "        \"Extracted_Symptom\": formatted_symptoms})\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    df_results.to_csv(\"Final_Extracting_one_symptom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8141660319878141"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91861ef7624f4757a1d082a5667c0610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d6b4bdac14d3993341478f9aef31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b2b8e3d15647a9b6c6822b59735e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d156245fb1d0455abaf6be368837110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21983957b8034d3a8039577372e79ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eafe33ae314bcb8df4285becff5583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a265d519674112925f3abb73a7e5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a35f4130a04e8ea0a97ceefba8988c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699da9d9e1284a8a9493a05ca7cba6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040fec39913f40ea87c536fe43e5794a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "model = LLM(model_name=\"meta-llama/Llama-3.2-3B-Instruct\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:19,  1.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "452it [13:15,  1.52s/it]"
     ]
    }
   ],
   "source": [
    "extractor = ExtractingPrompt(symptoms_list)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, phrase in tqdm(enumerate(df['Dialogue_Generated'])):\n",
    "\n",
    "    prompt = extractor.build_extraction_prompt(phrase)\n",
    "\n",
    "    symptoms_extracted_llm = model.generate_text(messages=prompt)\n",
    "\n",
    "    # Extract symptoms with scores > 0.80\n",
    "    symptom_scores = extract_symptom_scores(symptoms_extracted_llm)\n",
    "\n",
    "    symptoms_extracted = [symptom for symptom, score in symptom_scores.items() if score > 0.80]\n",
    "\n",
    "    # Format the output appropriately\n",
    "    if len(symptoms_extracted) == 0:\n",
    "        formatted_symptoms = None  # or \"\" if you prefer empty string\n",
    "    elif len(symptoms_extracted) == 1:\n",
    "        formatted_symptoms = symptoms_extracted[0]  # single symptom as string\n",
    "    else:\n",
    "        formatted_symptoms = \", \".join(symptoms_extracted)  # multiple as comma-separated string\n",
    "    \n",
    "\n",
    "    true_symptom = df['Symptom'][i]\n",
    "    \n",
    "    results.append({\n",
    "        \"Dialogue\": phrase,\n",
    "        \"True_Symptom\": true_symptom,\n",
    "        \"Extracted_Symptom\": formatted_symptoms})\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    df_results.to_csv(\"Final_Extracting_one_symptom_llama7b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('Final_Extracting_one_symptom_llama7b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4314159292035398"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
